{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.ensemble as ske\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn\n",
    "import utils\n",
    "import numpy as np\n",
    "import nltk\n",
    "import boto3\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(predicted_labels, actual_labels):\n",
    "    pos_mask = predicted_labels == 1\n",
    "    rec_mask = actual_labels == 1\n",
    "    precision = np.sum(actual_labels[pos_mask]) / len(predicted_labels[pos_mask])\n",
    "    recall = np.sum(predicted_labels[rec_mask]) / len(actual_labels[rec_mask])\n",
    "    print(str(precision) + \" \"  + str(recall))\n",
    "    return 2.0 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto3.client(service_name='comprehend', region_name='ap-southeast-1')\n",
    "\n",
    "def get_sentiment_score_aws(sentence_list):\n",
    "    score = 0\n",
    "    for sentence in sentence_list:\n",
    "        result = comprehend.detect_sentiment(Text=sentence, LanguageCode='en')\n",
    "        sentiment_info = result[\"SentimentScore\"]\n",
    "        if result[\"Sentiment\"] == \"POSITIVE\":\n",
    "            score = score + sentiment_info[\"Positive\"]\n",
    "        elif result[\"Sentiment\"] == \"NEGATIVE\":\n",
    "            score = score + sentiment_info[\"Negative\"] * -1\n",
    "        elif result[\"Sentiment\"] == \"NEUTRAL\":\n",
    "            score = score + sentiment_info[\"Neutral\"] * 0.3\n",
    "        else:\n",
    "            score = score + 0\n",
    "            #ignore the cases where the sentiment is mixed\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "The first step is to convert my data into a form that will make it easier to train classifiers on. I start with the _hygiene.dat.additional_ file since its already in CSV format and the easiest to read. I have converted the categories into an n-hot encoded vector that becomes a part of my features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(\"./Hygiene/hygiene.dat.additional\", header=None, names=[\"categories\", \"pincode\", \"review_count\", \"rating\"])\n",
    "features[\"categories\"] = features[\"categories\"].map(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Length: 546. Sample top 10: [1, 1, 1, 0, 0, 1, 1, 0, 0, 0]\n"
    }
   ],
   "source": [
    "#Reading the labels provided\n",
    "given_labels = []\n",
    "with open(\"./Hygiene/hygiene.dat.labels\", \"r\") as f:\n",
    "    for i in range(546):\n",
    "        given_labels.append(int(f.readline()))\n",
    "\n",
    "print(f\"Length: {len(given_labels)}. Sample top 10: {given_labels[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Getting the list of unique categories\n",
    "categories = set()\n",
    "for c in features[\"categories\"]:\n",
    "    t = set(c)\n",
    "    categories = categories.union(t)\n",
    "categories.remove(\"Restaurants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       pincode  review_count    rating  Modern European  Gluten-Free  Belgian  \\\n0        98118             4  4.000000            False        False    False   \n1        98109            21  4.047619            False        False    False   \n2        98103            14  3.111111            False        False    False   \n3        98112            42  4.088889            False        False    False   \n4        98102            12  3.071429            False        False    False   \n...        ...           ...       ...              ...          ...      ...   \n13294    98104             1  3.000000            False        False    False   \n13295    98116            29  4.258065            False        False    False   \n13296    98104             1  4.000000            False        False    False   \n13297    98109             2  4.000000            False        False    False   \n13298    98108             2  3.000000            False        False    False   \n\n       Irish  Indonesian  Australian  Afghan  ...  African  Chinese  \\\n0      False       False       False   False  ...    False    False   \n1      False       False       False   False  ...    False    False   \n2      False       False       False   False  ...    False    False   \n3      False       False       False   False  ...    False    False   \n4      False       False       False   False  ...    False    False   \n...      ...         ...         ...     ...  ...      ...      ...   \n13294  False       False       False   False  ...    False    False   \n13295  False       False       False   False  ...    False    False   \n13296  False       False       False   False  ...    False    False   \n13297  False       False       False   False  ...    False    False   \n13298  False       False       False   False  ...    False    False   \n\n       Mongolian   Soup  Szechuan  Southern  Tapas Bars  Cafes  Cajun/Creole  \\\n0          False  False     False     False       False  False         False   \n1          False  False     False     False       False  False         False   \n2          False  False     False     False       False  False         False   \n3          False  False     False     False       False  False         False   \n4          False  False     False     False       False  False         False   \n...          ...    ...       ...       ...         ...    ...           ...   \n13294      False  False     False     False       False  False         False   \n13295      False  False     False     False       False  False         False   \n13296      False  False     False     False       False  False         False   \n13297      False  False     False     False       False  False         False   \n13298      False  False     False     False       False  False         False   \n\n       Asian Fusion  \n0             False  \n1             False  \n2             False  \n3             False  \n4             False  \n...             ...  \n13294         False  \n13295         False  \n13296         False  \n13297         False  \n13298         False  \n\n[13299 rows x 101 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pincode</th>\n      <th>review_count</th>\n      <th>rating</th>\n      <th>Modern European</th>\n      <th>Gluten-Free</th>\n      <th>Belgian</th>\n      <th>Irish</th>\n      <th>Indonesian</th>\n      <th>Australian</th>\n      <th>Afghan</th>\n      <th>...</th>\n      <th>African</th>\n      <th>Chinese</th>\n      <th>Mongolian</th>\n      <th>Soup</th>\n      <th>Szechuan</th>\n      <th>Southern</th>\n      <th>Tapas Bars</th>\n      <th>Cafes</th>\n      <th>Cajun/Creole</th>\n      <th>Asian Fusion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>98118</td>\n      <td>4</td>\n      <td>4.000000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>98109</td>\n      <td>21</td>\n      <td>4.047619</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>98103</td>\n      <td>14</td>\n      <td>3.111111</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>98112</td>\n      <td>42</td>\n      <td>4.088889</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>98102</td>\n      <td>12</td>\n      <td>3.071429</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>13294</td>\n      <td>98104</td>\n      <td>1</td>\n      <td>3.000000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>13295</td>\n      <td>98116</td>\n      <td>29</td>\n      <td>4.258065</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>13296</td>\n      <td>98104</td>\n      <td>1</td>\n      <td>4.000000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>13297</td>\n      <td>98109</td>\n      <td>2</td>\n      <td>4.000000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>13298</td>\n      <td>98108</td>\n      <td>2</td>\n      <td>3.000000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>13299 rows × 101 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# n-hot encoding for categories\n",
    "for cat in categories:\n",
    "    features[cat] = cat in features[\"categories\"]\n",
    "\n",
    "#dropping the categories column since its not needed anymore\n",
    "features = features.drop(\"categories\", axis=1)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Data\n",
    "\n",
    "The reviews could also potentially provide additional insights into the hygiene of a restaurant. Since we are only interested about hygiene we need to extract the parts of the reviews that talk about hygiene. It is also not necessary that all reviews talk about hygiene. Initially I plan to use a simple method of counting hygiene related words in the review and rely on the rating of the restaurant to indicate whether the hygiene was good or bad. I understand that the rating is not just for the hygiene but I am assuming that if the restaurant was not clean that would be the overiding factor for the reviewer and that will help me classify the negative ones. The positive ones are a little bit trickier. In order to get the words related to hygiene I used the powerthesaurus.org to get the synonyms and antonyms of the word and treat them as seed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hygiene_rel_words = {\"sanitation\", \"salubrity\", \"sanitary\", \"hygienic\", \"tidiness\", \"sterility\", \"disinfection\", \"filth\", \"uncleanliness\", \"dirt\", \"garbage\", \"muck\", \"clean\", \"sterile\", \"dirty\", \"hygiene\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the sentiment scores from the AWS Comprehend API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenise_reviews(review_txt):\n",
    "    regex_tk = nltk.tokenize.RegexpTokenizer(pattern=r\"[.?!;,\\n]+\" ,gaps=True, discard_empty=True)\n",
    "    sentences = [s.strip() for s in regex_tk.tokenize(review_txt)]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "scores = []\n",
    "with open(\"./Hygiene/hygiene.dat\", \"r\") as rv_file:\n",
    "    for line in rv_file:\n",
    "        c = 0\n",
    "        sentences = sent_tokenise_reviews(line)\n",
    "        hygiene_rel_sent = []\n",
    "        for w in hygiene_rel_words:\n",
    "            for s in sentences:\n",
    "                if w in s:\n",
    "                    hygiene_rel_sent.append(s)\n",
    "        \n",
    "        scores.append(get_sentiment_score_aws(hygiene_rel_sent))\n",
    "\n",
    "features[\"hygiene_sentiment\"] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "type(False) == type(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       pincode  review_count    rating  Szechuan  Indonesian  Pakistani  \\\n0        98118             4  4.000000     False       False      False   \n1        98109            21  4.047619     False       False      False   \n2        98103            14  3.111111     False       False      False   \n3        98112            42  4.088889     False       False      False   \n4        98102            12  3.071429     False       False      False   \n...        ...           ...       ...       ...         ...        ...   \n13294    98104             1  3.000000     False       False      False   \n13295    98116            29  4.258065     False       False      False   \n13296    98104             1  4.000000     False       False      False   \n13297    98109             2  4.000000     False       False      False   \n13298    98108             2  3.000000     False       False      False   \n\n       Brazilian  Fondue  Latin American  Halal  ...  Chinese  Spanish  \\\n0          False   False           False  False  ...    False    False   \n1          False   False           False  False  ...    False    False   \n2          False   False           False  False  ...    False    False   \n3          False   False           False  False  ...    False    False   \n4          False   False           False  False  ...    False    False   \n...          ...     ...             ...    ...  ...      ...      ...   \n13294      False   False           False  False  ...    False    False   \n13295      False   False           False  False  ...    False    False   \n13296      False   False           False  False  ...    False    False   \n13297      False   False           False  False  ...    False    False   \n13298      False   False           False  False  ...    False    False   \n\n       Cambodian  Ethiopian  Vegan  Cajun/Creole  Sandwiches  Kosher  \\\n0          False      False  False         False       False   False   \n1          False      False  False         False       False   False   \n2          False      False  False         False       False   False   \n3          False      False  False         False       False   False   \n4          False      False  False         False       False   False   \n...          ...        ...    ...           ...         ...     ...   \n13294      False      False  False         False       False   False   \n13295      False      False  False         False       False   False   \n13296      False      False  False         False       False   False   \n13297      False      False  False         False       False   False   \n13298      False      False  False         False       False   False   \n\n       Salvadoran  hygiene_sentiment  \n0           False           0.000000  \n1           False           0.000000  \n2           False           0.000000  \n3           False           0.879902  \n4           False           0.000000  \n...           ...                ...  \n13294       False           0.000000  \n13295       False           0.000000  \n13296       False           0.000000  \n13297       False           0.000000  \n13298       False           0.000000  \n\n[13299 rows x 102 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pincode</th>\n      <th>review_count</th>\n      <th>rating</th>\n      <th>Szechuan</th>\n      <th>Indonesian</th>\n      <th>Pakistani</th>\n      <th>Brazilian</th>\n      <th>Fondue</th>\n      <th>Latin American</th>\n      <th>Halal</th>\n      <th>...</th>\n      <th>Chinese</th>\n      <th>Spanish</th>\n      <th>Cambodian</th>\n      <th>Ethiopian</th>\n      <th>Vegan</th>\n      <th>Cajun/Creole</th>\n      <th>Sandwiches</th>\n      <th>Kosher</th>\n      <th>Salvadoran</th>\n      <th>hygiene_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>98118</td>\n      <td>4</td>\n      <td>4.000000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>98109</td>\n      <td>21</td>\n      <td>4.047619</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>98103</td>\n      <td>14</td>\n      <td>3.111111</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>98112</td>\n      <td>42</td>\n      <td>4.088889</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.879902</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>98102</td>\n      <td>12</td>\n      <td>3.071429</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>13294</td>\n      <td>98104</td>\n      <td>1</td>\n      <td>3.000000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>13295</td>\n      <td>98116</td>\n      <td>29</td>\n      <td>4.258065</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>13296</td>\n      <td>98104</td>\n      <td>1</td>\n      <td>4.000000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>13297</td>\n      <td>98109</td>\n      <td>2</td>\n      <td>4.000000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>13298</td>\n      <td>98108</td>\n      <td>2</td>\n      <td>3.000000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>...</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>13299 rows × 102 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "features = pd.read_csv(\"./output/data.csv\", index_col=0)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./Hygiene/hygiene.dat\", \"r\", encoding=\"utf-8\") as rv_file:\n",
    "    reviews = pd.Series([line.strip() for line in rv_file])\n",
    "\n",
    "with open(\"stopwords.txt\", \"r\", encoding=\"utf-8\") as st:\n",
    "    stopwords = {line.strip() for line in st}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = sklearn.feature_extraction.text.CountVectorizer(stop_words=stopwords, max_features=1000)\n",
    "word_feat = vec.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0, 0, 1, ..., False, False, 0.0],\n       [1, 0, 1, ..., False, False, 0.0],\n       [2, 0, 1, ..., False, False, 0.0],\n       ...,\n       [0, 0, 0, ..., False, False, 0.0],\n       [0, 0, 0, ..., False, False, 0.0],\n       [0, 0, 0, ..., False, False, 0.0]], dtype=object)"
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "features = np.hstack((word_feat.toarray(), features.to_numpy()))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i, col in enumerate(vec.get_feature_names()):\n",
    "    features[col] = pd.Series(word_feat[:, i].toarray().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Decision Tree trained with given labels\n",
    "\n",
    "Now I will attempt to train a basic decision tree on the given labels using just these features as input. Not considering the review data yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=None, splitter='best')"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "training_data = features[:546]\n",
    "labels = pd.Series(given_labels)\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(training_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.0 1.0\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.0"
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "calculate_f1_score(classifier.predict(training_data), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(features[546:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Submission completed successfully!\n"
    }
   ],
   "source": [
    "utils.pred_save_submit_to_leaderboard(predictions, \"dt.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosted Decision tree\n",
    "\n",
    "This time we train a boosted tree on the same data and see if that does better. AdaBoost is the algorithm used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-fbd1fbbb78d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# by default this implementation uses a DecisionTreeClassifier with a max depth of 1. Since we have very few features I didn't want to increase the depth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mboost_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mske\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m350\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mboost_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m546\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m                 random_state)\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \"\"\"\n\u001b[0;32m    485\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0miboost\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    845\u001b[0m         \"\"\"\n\u001b[0;32m    846\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tree_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;34m\"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[0;32m    393\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# by default this implementation uses a DecisionTreeClassifier with a max depth of 1. Since we have very few features I didn't want to increase the depth\n",
    "boost_clf = ske.AdaBoostClassifier(n_estimators=350)\n",
    "boost_clf.fit(features[:546], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Submission completed successfully!\n"
    }
   ],
   "source": [
    "utils.pred_save_submit_to_leaderboard(boost_clf.predict(features[546:]), \"adaboost.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method yielded a Precision of 0.5544 and a Recall of 0.5573. So a small improvement is observed. Next lets also try a random forest. I don't expect the results to be better than AdaBoost since boosting actively tries to fix its mistakes for the subsequent classifiers but I do expect it to do better that a single decision tree.\n",
    "\n",
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=700,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)"
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "rf_clf = ske.RandomForestClassifier(n_estimators=700)\n",
    "rf_clf.fit(features[:546], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Submission completed successfully!\n"
    }
   ],
   "source": [
    "utils.pred_save_submit_to_leaderboard(rf_clf.predict(features[546:]), \"random_forest.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None,\n                                                        criterion='gini',\n                                                        max_depth=None,\n                                                        max_features=None,\n                                                        max_leaf_nodes=None,\n                                                        min_impurity_decrease=0.0,\n                                                        min_impurity_split=None,\n                                                        min_samples_leaf=1,\n                                                        min_samples_split=2,\n                                                        min_weight_fraction_leaf=0.0,\n                                                        presort=False,\n                                                        random_state=None,\n                                                        splitter='best'),\n                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n                  max_samples=1.0, n_estimators=700, n_jobs=None,\n                  oob_score=False, random_state=None, verbose=0,\n                  warm_start=False)"
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "source": [
    "bagg_clf = ske.BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=700)\n",
    "bagg_clf.fit(features[:546], labels)\n",
    "#calculate_f1_score(bagg_clf.predict(features[:546]), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Submission completed successfully!\n"
    }
   ],
   "source": [
    "utils.pred_save_submit_to_leaderboard(bagg_clf.predict(features[546:]), \"bagg_dt.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Submission completed successfully!\n"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive_b = GaussianNB()\n",
    "naive_b.fit(training_data, labels)\n",
    "\n",
    "utils.pred_save_submit_to_leaderboard(naive_b.predict(features[546:]), \"nb.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False)"
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "lr_clf = sklearn.linear_model.LogisticRegression()\n",
    "lr_clf.fit(features[:546], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Submission completed successfully!\n"
    }
   ],
   "source": [
    "utils.pred_save_submit_to_leaderboard(lr_clf.predict(features[546:]), \"lr.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using unlabelled data\n",
    "\n",
    "Next lets use some of the methods that we learnt for training models when the amount of unlabelled data is much more than that of the labelled data. Here I'll use the method of running a trained classfier on the unlabelled data and then adding the ones with the highest confidence back to the training set. Also, I'll use the records which have _hygiene_word_count_ > 0 but I'll make sure not add more than 50% of the total records to the training set to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(4150, 102)"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "unlabelled_data = features[546:]\n",
    "unlabelled_data_for_training = unlabelled_data[unlabelled_data[\"hygiene_sentiment\"] > 0]\n",
    "unlabelled_data_for_training.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the total number of unlabelled examples I want to use for training is 13300/2 = 6650, I need to add 6650-4150=2500 samples to this set from the unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initial training data: (546, 1101), Type: <class 'pandas.core.frame.DataFrame'>\niter: 1 Remaining unlabelled data: 3804\niter: 2 Remaining unlabelled data: 3619\niter: 3 Remaining unlabelled data: 3377\niter: 4 Remaining unlabelled data: 3137\niter: 5 Remaining unlabelled data: 2934\niter: 6 Remaining unlabelled data: 2733\niter: 7 Remaining unlabelled data: 2562\niter: 8 Remaining unlabelled data: 2379\niter: 9 Remaining unlabelled data: 2263\niter: 10 Remaining unlabelled data: 2154\niter: 11 Remaining unlabelled data: 2059\niter: 12 Remaining unlabelled data: 1982\niter: 13 Remaining unlabelled data: 1899\niter: 14 Remaining unlabelled data: 1845\niter: 15 Remaining unlabelled data: 1803\niter: 16 Remaining unlabelled data: 1759\niter: 17 Remaining unlabelled data: 1710\niter: 18 Remaining unlabelled data: 1668\niter: 19 Remaining unlabelled data: 1606\niter: 20 Remaining unlabelled data: 1563\niter: 21 Remaining unlabelled data: 1530\niter: 22 Remaining unlabelled data: 1479\niter: 23 Remaining unlabelled data: 1386\niter: 24 Remaining unlabelled data: 1274\niter: 25 Remaining unlabelled data: 1194\niter: 26 Remaining unlabelled data: 1148\niter: 27 Remaining unlabelled data: 1112\niter: 28 Remaining unlabelled data: 1090\niter: 29 Remaining unlabelled data: 1070\niter: 30 Remaining unlabelled data: 1053\niter: 31 Remaining unlabelled data: 1041\niter: 32 Remaining unlabelled data: 1033\niter: 33 Remaining unlabelled data: 1025\niter: 34 Remaining unlabelled data: 1017\niter: 35 Remaining unlabelled data: 1006\niter: 36 Remaining unlabelled data: 997\niter: 37 Remaining unlabelled data: 992\niter: 38 Remaining unlabelled data: 990\niter: 39 Remaining unlabelled data: 983\niter: 40 Remaining unlabelled data: 979\niter: 41 Remaining unlabelled data: 977\niter: 42 Remaining unlabelled data: 970\niter: 43 Remaining unlabelled data: 969\niter: 44 Remaining unlabelled data: 965\niter: 45 Remaining unlabelled data: 961\niter: 46 Remaining unlabelled data: 957\niter: 47 Remaining unlabelled data: 955\niter: 48 Remaining unlabelled data: 953\niter: 49 Remaining unlabelled data: 952\nTerminating\n"
    }
   ],
   "source": [
    "training_data = features[:546]\n",
    "unlabelled_data = features[546:]\n",
    "unlabelled_data_for_training = unlabelled_data[unlabelled_data[\"hygiene_sentiment\"] < 0]\n",
    "unlabelled_data_for_training = unlabelled_data_for_training.append(unlabelled_data[unlabelled_data[\"hygiene_sentiment\"] >= 0].sample(2500))\n",
    "curr_training_set = training_data\n",
    "curr_labels = pd.Series(given_labels)\n",
    "\n",
    "print(f\"Initial training data: {training_data.shape}, Type: {type(training_data)}\")\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "self_train_clf = ske.RandomForestClassifier(n_estimators=700)\n",
    "#self_train_clf = ske.BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=150)\n",
    "\n",
    "while unlabelled_data_for_training.shape[0] > 0:\n",
    "    self_train_clf.fit(curr_training_set, curr_labels)\n",
    "    probs = self_train_clf.predict_proba(unlabelled_data_for_training)\n",
    "    results = self_train_clf.predict(unlabelled_data_for_training)\n",
    "    high_conf = (probs[:,0] >= 0.8) | (probs[:,1] >= 0.8)\n",
    "\n",
    "    if len(results[high_conf]) < 10:\n",
    "        print(\"Terminating\")\n",
    "        break\n",
    "    curr_training_set = curr_training_set.append(unlabelled_data_for_training[high_conf])\n",
    "    curr_labels = curr_labels.append(pd.Series(results[high_conf]))\n",
    "\n",
    "    unlabelled_data_for_training = unlabelled_data_for_training[np.invert(high_conf)]\n",
    "    iterations = iterations + 1\n",
    "    print(f\"iter: {iterations} Remaining unlabelled data: {unlabelled_data_for_training.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use this classifier to make predictions... It should be significantly better than the previous ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = self_train_clf.predict(features[546:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Submission completed successfully!\n"
    }
   ],
   "source": [
    "utils.pred_save_submit_to_leaderboard(predictions, \"self_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}